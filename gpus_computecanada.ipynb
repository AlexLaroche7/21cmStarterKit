{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Using GPUs on Compute Canada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Accounts: Helios or Beluga? You get to choose!\n",
    "You can choose to work on Helios or Beluga with GPUs. If you want to use Helios, you'll first have to get an account with Calcul Qu√©bec (See step 3 after you have Compute Canada's accounts all set up https://www.calculquebec.ca/en/academic-research-services/procedures/)\n",
    "\n",
    "You can log onto Helios like this, remembering that your Calcul Quebec password may be different than your Compute Canada one:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helios:\n",
    "ssh <username>@helios.calculquebec.ca\n",
    "\n",
    "## Beluga:\n",
    "ssh <username>@beluga.computecanada.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) job scripts and setup\n",
    "\n",
    "As you do when submitting jobs using multiple CPUs, you must also submit a job script when running on GPUs.\n",
    "\n",
    "Compute Canada uses the resource manager/job scheduler, Slurm (https://support.ceci-hpc.be/doc/_contents/QuickStart/SubmittingJobs/SlurmTutorial.html). \n",
    "\n",
    "## a) Helios: Checking whether CUDA can be accessed\n",
    "(scripts thanks to Christine!)\n",
    "\n",
    "You'll want to first find your RAP ID with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helios-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in hand, You'll be able to request up to 8 GPUs/node for Helios K20 nodes and 16 GPUs/node on Helios K80 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test.sh: sample job script to run GPUs on Helios\n",
    "#!/bin/bash\n",
    "#PBS -N <Name your job here>\n",
    "#PBS -A <Your RAP ID goes here>\n",
    "#PBS -l walltime=300\n",
    "#PBS -l nodes=1:gpus=2 \n",
    "cd \"${PBS_O_WORKDIR}\"\n",
    "python test.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test.py: sample script which checks to see whether CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "CUDA = t.cuda.is_available()\n",
    "print(CUDA)\n",
    "\n",
    "### This should error when you run it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cd command into ${PBS_O_WORKDIR} makes sure to execute the file where the job is submitted.\n",
    "\n",
    "Hopefully, this script will return True after you submit it with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sbatch test.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Beluga: tensorflow-gpu / Keras \n",
    "\n",
    "Do you want to accelerate your machine learning training to less than half the amount of time it takes on one CPU (or more? I don't really know the scaling, but you get to find out!)? Well this is your section!\n",
    "\n",
    "After logging onto Beluga, you're going to want to load your favorite python version, e.g. Python 3.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "module load python/3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you're going to want to create a Python virtual environment that you can activate before you start your job. You'll probably want to create your environment in the PROJECT storage area (1TB of space/group)\n",
    "\n",
    "My project folder path is like this: /project/def-acliu/sabrinab\n",
    "\n",
    "Create a virtual environment called test-env (do change this name when you make yours!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "virtualenv --no-download test-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate your virtual environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source test-env/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the GPU version of tensorflow:\n",
    "\n",
    "Be careful here! Are you using tensorflow 2.0 already? Wow, you are way ahead of the game if you do. I'm not. So I have to specify the tensorflow version that I want. Beluga already has a bunch pre-configured and available so try installing the version you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install --no-index tensorflow_gpu==1.14.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The no-index flag ensures that you're installing a package that was compiled by Compute Canada people and can potentially help with missing/conflicting dependencies.\n",
    "\n",
    "If you're using the Keras framework, you'll also have to install that with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install --no-index keras == your version here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test.sh: sample job script to run a GPU on Beluga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --gres=gpu:1        # request GPU \"generic resource\"\n",
    "#SBATCH --cpus-per-task=6   # maximum CPU cores per GPU request: 6 on Cedar, 16 on Graham.\n",
    "#SBATCH --mem=128G      # memory per node (may change depending how much memory your script needs)\n",
    "#SBATCH --time=0-1:00   # time (DD-HH:MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source /project/def-acliu/sabrinab/test-env/bin/activate\n",
    "python /project/def-acliu/sabrinab/train.py\n",
    "\n",
    "Try a sample job script submission like the one above to run train.py on a GPU. Surprisingly, easy! This saved us from having to learn CUDA and directly interact with the GPU. We'll save that for next time... or maybe the next life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
